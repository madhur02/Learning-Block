{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM** in its core, preserves information from inputs that has already passed through it using the hidden state.\n",
    "\n",
    "**Unidirectional LSTM** only preserves information of the past because the only inputs it has seen are from the past.\n",
    "\n",
    "**Using bidirectional** will run your inputs in two ways, one from past to future and one from future to past and what differs this approach from unidirectional is that in the LSTM that runs backwards you preserve information from the future and using the two hidden states combined you are able in any point in time to preserve information from both past and future.\n",
    "\n",
    "Lets say we try to predict the next word in a sentence, on a high level what a unidirectional LSTM will see is\n",
    "\n",
    "The boys went to ....\n",
    "\n",
    "And will try to predict the next word only by this context, with bidirectional LSTM you will be able to see information further down the road for example\n",
    "\n",
    "Forward LSTM:\n",
    "\n",
    "The boys went to ...\n",
    "\n",
    "Backward LSTM:\n",
    "\n",
    "... and then they got out of the pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How you would implement Bidirectional LSTM without calling BiLSTM module. \n",
    "\n",
    "This might better contrast the difference between a uni-directional and bi-directional LSTMs. As you see, we merge two LSTMs to create a bidirectional LSTM.\n",
    "\n",
    "You can merge outputs of the forward and backward LSTMs by using either {'sum', 'mul', 'concat', 'ave'}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = Sequential()\n",
    "left.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',\n",
    "               forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "               inner_activation='sigmoid', input_shape=(99, 13)))\n",
    "right = Sequential()\n",
    "right.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',\n",
    "               forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "               inner_activation='sigmoid', input_shape=(99, 13), go_backwards=True))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([left, right], mode='sum'))\n",
    "\n",
    "model.add(TimeDistributedDense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-5, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "print(\"Train...\")\n",
    "model.fit([X_train, X_train], Y_train, batch_size=1, nb_epoch=nb_epoches, validation_data=([X_test, X_test], Y_test), verbose=1, show_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM or BLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to LSTM, BLSTM or BiLSTM has two networks, one access pastinformation in forward direction and another access future in the reverse direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5,10)))\n",
    "\n",
    "# activation function can be added like this:\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(num_channels,implementation = 2, recurrent_activation = 'sigmoid'),input_shape=(input_length, input_dim)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complte model using Bi-directional LSTM on imdb dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "n_unique_words = 10000 # cut texts after this number of words\n",
    "maxlen = 200\n",
    "batch_size = 128\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=n_unique_words)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_unique_words, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=4,\n",
    "          validation_data=[x_test, y_test])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
